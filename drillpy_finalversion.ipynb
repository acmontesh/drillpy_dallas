{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy A.I. Hackathon 2023 - Project Template \n",
    "\n",
    "## General Guidance\n",
    "\n",
    "We're expecting a workflow that could be deployed to any competent engineer or scientist with basic subsurface resource, data analytics and machine learning knowledge and they could understand and apply your workflow. \n",
    "\n",
    "### Expectations on the Workflow\n",
    "\n",
    "* include short descriptions, no 2 code blocks should be adjacent, always have a short statement to explain the next code block\n",
    "\n",
    "* be as concise as possible:\n",
    "\n",
    "    * use point form (except for the executive summary) \n",
    "    * use effective, creative figures that compine what could have been in multiple plots\n",
    "    * every line of code, statment or figure must have purpose\n",
    "    * conciseness is part of the grading, don't add content that isn't needed\n",
    "    \n",
    "* be very clear with readable code\n",
    "\n",
    "    * label every axis for every plot\n",
    "    * use readable code, logical variable names, use available functionality and define functions and classes for compactness and concise comments in the code\n",
    "    * proceed step by step, explain each important step concisely for a easy to follow narrative \n",
    "    \n",
    "  \n",
    "### Using Code From Others\n",
    "  \n",
    "You may use blocks/snipets of code from other sources with citation. To cite a set of code separate in a block and do this in the markdown above the block.\n",
    "\n",
    "The following code block is from Professor Michael Pyrcz (@GeostatsGuy), SubSurfuceDataAnalytics_PCA.ipynb from [GeostatsGuy GitHub](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PCA.ipynb).\n",
    "\n",
    "```python\n",
    "def simple_simple_krige(df,xcol,ycol,vcol,dfl,xlcol,ylcol,vario,skmean):\n",
    "# load the variogram\n",
    "    nst = vario['nst']; pmx = 9999.9\n",
    "    cc = np.zeros(nst); aa = np.zeros(nst); it = np.zeros(nst)\n",
    "```\n",
    "\n",
    "or use inline citations with comments, such as this for a few of lines of code.\n",
    "\n",
    "```python\n",
    "def simple_simple_krige(df,xcol,ycol,vcol,dfl,xlcol,ylcol,vario,skmean): # function from Professor Michael Pyrcz,https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_PCA.ipynb \n",
    "```\n",
    "\n",
    "## The Workflow Template\n",
    "\n",
    "Here's the template for your workflow.\n",
    "\n",
    "___\n",
    "\n",
    "# Energy A.I. Hackathon 2023 Workflow - Drillpy \n",
    "\n",
    "#### Authors: Abraham Montes, Ethan Litchauer, Mohammad Abdullah, Çınar Turhan \n",
    "#### Petroleum and Geosystems Engineering, Electrical and Computer Engineering\n",
    "\n",
    "#### The University of Texas at Austin, Austin, Texas USA \n",
    "___\n",
    "\n",
    "### Executive Summary \n",
    "\n",
    "This will require:\n",
    "\n",
    "Data analysis and evaluation of multiple data sources and a variety of features\n",
    "feature engineering including feature selection, feature transformations and feature imputation to address missing data\n",
    "integration of domain expertise at every step\n",
    "selection, training and tuning robust machine learning prediction models\n",
    "\n",
    "Goal: Develop a data analytics and machine learning workflow in Python to:\n",
    "\n",
    "prediction / classification \"fail\" or \"not fail\" within 30 days for 40 artificial lift Electronic Submersible Pumps\n",
    "Background\n",
    "In order to prevent the production loss caused by Electronic Submersible Pump (ESP) failures in artificial lift operations, we challenge the Energy A.I. Hackathon 2023 teams of The University of Texas at Austin to build a data-driven model to detect when an ESP is within 30 days of failure.\n",
    "\n",
    "\n",
    "ESP failures can result in significant production loss and pose environmental and safety risks. One way to predict these failures to prevent monetary and safety consequences is using a machine learning model. In this workflow, we analyze two separate datasets. The first one relates to the technical properties of various ESPs, and the second is dynamic data consisting of many features, from oil production to the ratio of pump power to drive power. \n",
    "___\n",
    "\n",
    "### Workflow Goal\n",
    "\n",
    "Analyze the data, simplify the data to useful features, select the best model for accuracy.\n",
    "___\n",
    "\n",
    "### Workflow Steps \n",
    "\n",
    "Enumerated steps, very short and concise overview of your methods and workflow\n",
    "\n",
    "1. **Data Analysis** - basic data checking and visualization\n",
    "2. **Feature Selection** - mutual information-based approach with minimum redundancy, maximum relevance score\n",
    "3. **Machine Learning Model \\#1** - Predict feature $X$ from $Y,Z$\n",
    "\n",
    "$\\ldots$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "### Reading Files\n",
    "df_daily = pd.read_csv(\"dailyData.csv\")\n",
    "df_daily.groupby([\"Well_ID\", \"AL_Key\"])\n",
    "df_daily.loc[df_daily[\"Well_ID\"] == 15]\n",
    "df_daily = df_daily[(df_daily[\"OIL\"] >= 10) & (df_daily[\"WATER\"] >= 10) & (df_daily[\"GAS\"] >= 10)]\n",
    "### Functions and Utilities\n",
    "def plot_window(df, channelName, wellID):\n",
    "    import matplotlib.pyplot as plt    \n",
    "    fig,ax = plt.subplots(figsize=(15,5))\n",
    "    ax.plot(df[df[\"Well_ID\"]==wellID][channelName].iloc[-90:-1])\n",
    "    ax.axvline(x=df[df[\"Well_ID\"]==wellID].shape[0]-30, c=\"orange\", ls=\"--\")\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(df[df[\"Well_ID\"]==wellID][channelName].iloc[-90:-1])\n",
    "\n",
    "\n",
    "### Plotting\n",
    "plot_window(df_daily, \"OIL\", 345)\n",
    "fig,ax1 = plt.subplots(figsize=(15,5))\n",
    "#ax.plot(df_daily[df_daily[\"Well_ID\"]==165][\"ESP Data - Motor Temperature Shutdown Setpoint\"], c=\"blue\")\n",
    "#ax.axvline(x=df_daily[df_daily[\"Well_ID\"]==165].shape[0]-30, c=\"orange\", ls=\"--\")\n",
    "#ax1 = ax.twinx()\n",
    "ax1.plot(df_daily[df_daily[\"Well_ID\"]==165][\"ESP Data - Motor Winding Temperature\"], c=\"green\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_daily[df_daily[\"Well_ID\"]==165][\"DOWN_TIME_HOURS\"], c=\"red\")\n",
    "ax3 = ax1.twinx()\n",
    "ax2.plot(df_daily[df_daily[\"Well_ID\"]==165][\"ESP Data - Motor Temperature Shutdown Setpoint\"], c=\"blue\")\n",
    "fig,ax1 = plt.subplots(figsize=(15,5))\n",
    "#ax.plot(df_daily[df_daily[\"Well_ID\"]==165][\"ESP Data - Motor Temperature Shutdown Setpoint\"], c=\"blue\")\n",
    "#ax.axvline(x=df_daily[df_daily[\"Well_ID\"]==165].shape[0]-30, c=\"orange\", ls=\"--\")\n",
    "#ax1 = ax.twinx()\n",
    "ax1.plot(df_daily[df_daily[\"Well_ID\"]==165][\"ESP Data - Motor Winding Temperature\"], c=\"green\")\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_daily[df_daily[\"Well_ID\"]==165][\"DOWN_TIME_HOURS\"], c=\"red\")\n",
    "#df_null = df_daily[\"WATER\"].isnull().count()/df_daily[\"WATER\"].count()\n",
    "#df_water = df_daily[\"WATER\"].isnull().shape[0]/df_daily[\"WATER\"].shape[0]\n",
    "percent_missing = df_daily.isnull().sum() * 100 / len(df_daily)\n",
    "#df_total = [df_daily[i].count() for i in df_daily.columns]\n",
    "#df_array = df_new.to_numpy()\n",
    "#plt.bar(df_null, df_total)\n",
    "print(percent_missing)\n",
    "#fig, ax = plt.subplots(figsize =(10, 10))\n",
    "#ax.hist(percent_missing, len(df_daily.axes[1]), df_daily.columns)\n",
    "#plt.hist(percent_missing, len(df_daily.axes[1]), df_daily.columns.astype(\"string\"))\n",
    "#plt.show()\n",
    "#ticks = range(len(df_daily.columns))\n",
    "#plt.bar(ticks,len(df_daily.columns), align='center')\n",
    "#plt.xticks(ticks, df_daily.columns)\n",
    "#dailyData = sns.load_dataset(df_daily).pivot(\"Model\", \"Task\", \"Score\")\n",
    "#sns.heatmap(df_daily)\n",
    "#plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_daily.isna().transpose().drop([\"AL_Key\", \"Well_ID\"]),\n",
    "            cmap=\"YlGnBu\",\n",
    "            cbar_kws={'label': 'Missing Data'})\n",
    "sns.displot(\n",
    "    data=df_daily.isna().melt(value_name=\"Is this missing?\"),\n",
    "    y=\"variable\",\n",
    "    hue=\"Is this missing?\",\n",
    "    multiple=\"fill\",\n",
    "    aspect=1.25\n",
    ")\n",
    "df_new = pd.DataFrame({\"Percent Missing\": percent_missing}, index = df_daily.columns)\n",
    "ax = df_new.plot.bar(rot = 90)\n",
    "df_new_daily = pd.DataFrame()\n",
    "df_new_daily = df_daily.iloc[:, [4, 5, 6, 9, 10, 15, 21, 27, 33, 34, 20, 22]].copy()\n",
    "df_new_daily[\"WOR\"] = df_daily[\"WATER\"]/df_daily[\"OIL\"]\n",
    "df_new_daily[\"GOR\"] = df_daily[\"GAS\"]/df_daily[\"OIL\"]\n",
    "df_new_daily[\"Well_ID\"] = df_daily.iloc[:, [ -1]].copy()\n",
    "df_new_daily[\"AL_Key\"] = df_daily.iloc[:, [ -2]].copy()\n",
    "print(df_new_daily)\n",
    "df_new_daily.head()\n",
    "#9 vs 34\n",
    "#15 vs 22\n",
    "print('Daily Data Pearson Correlation Coefficient')\n",
    "sns.set(font_scale=0.6)\n",
    "_=sns.heatmap(df_new_daily.corr(), annot=True, cmap='magma')\n",
    "df_new_daily2 = df_new_daily.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15]]\n",
    "#df_new_daily2.drop([10, 11], axis=1, inplace=True)\n",
    "#df_new_daily2.concat(df_new_daily.iloc[:, 12:14])\n",
    "df_new_daily2.head()\n",
    "#take out downtime. boxplot to see anomalies. impute missing data with linear regression. correlation coefficients again.\n",
    "df_new_daily3 = df_new_daily2.iloc[:, [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
    "df_new_daily3.head()\n",
    "from sklearn.preprocessing import (PolynomialFeatures, MaxAbsScaler,\n",
    "                                   MinMaxScaler, StandardScaler, RobustScaler)\n",
    "#### scaling method ###\n",
    "df_nd3_null = df_new_daily3.isnull().copy()\n",
    "scalers = [None,MaxAbsScaler(), MinMaxScaler(), StandardScaler(), RobustScaler()]\n",
    "x = df_new_daily3[~df_nd3_null].to_numpy()\n",
    "#print(x)\n",
    "d1 = df_new_daily3[pd.isnull(df_new_daily3[:])]\n",
    "d1.head(30)\n",
    "#scaleit = MinMaxScaler()\n",
    "#print(scaleit.fit(x))\n",
    "#MinMaxScaler()\n",
    "#print(scaleit.data_max_)\n",
    "df_new_daily3.boxplot(rot = 90)\n",
    "sns.pairplot(df_new_daily3)\n",
    "df_new_daily3.hist(\"Gas_Intake\", bins = 100)\n",
    "df_test1 = df_new_daily3.iloc[:, [0, 1, 2]].copy()\n",
    "df_test2 = df_new_daily3.iloc[:, 3:6].copy()\n",
    "df_test3 = df_new_daily3.iloc[:, 6:10].copy()\n",
    "sns.pairplot(df_test1)\n",
    "sns.pairplot(df_test2)\n",
    "sns.pairplot(df_test3)\n",
    "# This is based on data analytics per plot and data distribution. \n",
    "# Before was engineering, now based on data analytics and outlier detection\n",
    "df_new_daily4 = \\\n",
    "    df_new_daily3[(df_new_daily3[\"GOR\"] < 40) & (df_new_daily3[\"ESP Data - Drive Current\"] > 0) & \n",
    "    (df_new_daily3[\"ESP Data - Drive Voltage\"] > 0) & (df_new_daily3[\"Pump_Delta_Pressure\"] < 3600) & \n",
    "    (df_new_daily3[\"Gas_through_Annulus\"] < 800) & (df_new_daily3[\"WOR\"] < 100)\n",
    "    & (df_new_daily3[\"Power_Difference\"] < 200)]\n",
    "df_new_daily4.head()\n",
    "df_test12 = df_new_daily4.iloc[:, [0, 1, 2]].copy()\n",
    "df_test22 = df_new_daily4.iloc[:, 3:6].copy()\n",
    "df_test32 = df_new_daily4.iloc[:, 6:10].copy()\n",
    "sns.pairplot(df_test12)\n",
    "sns.pairplot(df_test22)\n",
    "sns.pairplot(df_test32)\n",
    "df_new_daily5 = \\\n",
    "    df_new_daily4[(df_new_daily4[\"WOR\"] < 10) & (df_new_daily4[\"GOR\"] < 12) & (df_new_daily4[\"ESP Data - Drive Voltage\"] > 200)]\n",
    "    #df_new_daily4[(df_new_daily4[\"WOR\"] < 10) & (df_new_daily4[\"GOR\"] < 12) & (df_new_daily4[\"ESP Data - Drive Voltage\"] < 200)]\n",
    "df_test13 = df_new_daily5.iloc[:, [0, 1, 2]].copy()\n",
    "df_test23 = df_new_daily5.iloc[:, 3:6].copy()\n",
    "df_test33 = df_new_daily5.iloc[:, 6:10].copy()\n",
    "#sns.pairplot(df_test13)\n",
    "#sns.pairplot(df_test23)\n",
    "#sns.pairplot(df_test33)\n",
    "df_new_daily5.head()\n",
    "df_new_daily6 = \\\n",
    "    df_new_daily5[(df_new_daily5[\"ESP Data - Drive Voltage\"] > 250) & (df_new_daily5[\"GOR\"] < 8)]\n",
    "    #df_new_daily4[(df_new_daily4[\"WOR\"] < 10) & (df_new_daily4[\"GOR\"] < 12) & (df_new_daily4[\"ESP Data - Drive Voltage\"] < 200)]\n",
    "df_test14 = df_new_daily6.iloc[:, [0, 1, 2]].copy()\n",
    "df_test24 = df_new_daily6.iloc[:, 3:6].copy()\n",
    "df_test34 = df_new_daily6.iloc[:, 6:10].copy()\n",
    "#sns.pairplot(df_test14)\n",
    "#sns.pairplot(df_test24)\n",
    "#sns.pairplot(df_test34)\n",
    "df_new_daily6.head()\n",
    "sns.displot(\n",
    "    data=df_new_daily6.isna().melt(value_name=\"Is this missing?\"),\n",
    "    y=\"variable\",\n",
    "    hue=\"Is this missing?\",\n",
    "    multiple=\"fill\",\n",
    "    aspect=1.25\n",
    ")\n",
    "#df_nd6_null = df_new_daily6.isnull().copy()\n",
    "#x = df_new_daily6[~df_nd6_null].to_numpy()\n",
    "#print(x)\n",
    "#d6 = df_new_daily6[pd.isnull(df_new_daily6[:])]\n",
    "#d6.describe().T\n",
    "df_new_daily6.info()\n",
    "df_new_daily6.count()\n",
    "df_new_daily7 = df_new_daily6.copy()\n",
    "df_new_daily7 = df_new_daily7.dropna()\n",
    "df_new_daily7.to_csv(\"final_dataset.csv\")\n",
    "df_daily.shape\n",
    "df_any = pd.read_csv(\"wellData.csv\")\n",
    "df_any.shape\n",
    "\n",
    "### Machine Learning Workflow\n",
    "### Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "### Reading Files\n",
    "df_daily = pd.read_csv(\"final_dataset.csv\")\n",
    "### Functions and Utilities\n",
    "#Plots the 30 days window before last failure\n",
    "def plot_window(df, channelName, wellID):\n",
    "    import matplotlib.pyplot as plt    \n",
    "    fig,ax = plt.subplots(figsize=(15,5))\n",
    "    ax.plot(df[df[\"Well_ID\"]==wellID][channelName].iloc[-90:-1])\n",
    "    ax.axvline(x=df[df[\"Well_ID\"]==wellID].shape[0]-30, c=\"orange\", ls=\"--\")\n",
    "\n",
    "#Slide the window over each \"Pump history\" and build dataset\n",
    "def slide_window(df, windowSize, step=1):\n",
    "    import pandas as pd\n",
    "    import scipy.stats as sps\n",
    "    import numpy as np\n",
    "    #dropping IDs\n",
    "    df_local = df.copy(deep=True)\n",
    "    df_local = df_local.drop([\"Well_ID\", \"AL_Key\"], axis=1)\n",
    "    #rolling window\n",
    "    out_df_mean = df_local.rolling(windowSize, step=step).mean().tail(-windowSize+1)\n",
    "    out_df_std = df_local.rolling(windowSize, step=step).std().tail(-windowSize+1)\n",
    "    #out_df_kur = df_local.rolling(windowSize, step=step).apply(sps.kurtosis).tail(-windowSize+1)\n",
    "    #rename columns before merging\n",
    "    out_df_mean.columns=[out_df_mean.columns[i] + \"_mean\" for i in range(len(out_df_mean.columns))]\n",
    "    out_df_std.columns=[out_df_std.columns[i] + \"_std\" for i in range(len(out_df_std.columns))]\n",
    "    #out_df_kur.columns=[out_df_kur.columns[i] + \"_kur\" for i in range(len(out_df_kur.columns))]\n",
    "    #Stack DFs\n",
    "    out_df = pd.concat([out_df_mean,out_df_std], axis=1).reindex(out_df_mean.index)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "#Builds the dataset, sliding window for each well and each pump\n",
    "def build_dataset(df, windowSize, df_solutions, step=1):\n",
    "    import pandas as pd\n",
    "    wells = df[\"Well_ID\"].unique()\n",
    "    j=0\n",
    "    solData = pd.DataFrame()\n",
    "    for well in wells:\n",
    "        pumps = df[df[\"Well_ID\"]==well][\"AL_Key\"].unique()\n",
    "        i=0\n",
    "        for pump in pumps:\n",
    "            bufferDF = df[(df[\"Well_ID\"]==well) & (df[\"AL_Key\"]==pump)]\n",
    "            if bufferDF.shape[0]>windowSize*step:\n",
    "                print(bufferDF.shape[0])\n",
    "                if i==0:\n",
    "                    pieceDF = slide_window(bufferDF, windowSize, step)\n",
    "                    rows = pieceDF.shape[0]\n",
    "                    fails = np.array([0]*rows)\n",
    "                    fails[-1]=1\n",
    "                    pieceDF[\"fail\"] = fails\n",
    "                    \n",
    "                else:\n",
    "                    dummyDF =  slide_window(bufferDF, windowSize, step)\n",
    "                    rows = dummyDF.shape[0]\n",
    "                    fails = np.array([0]*rows)\n",
    "                    fails[-1]=1\n",
    "                    dummyDF[\"fail\"] = fails\n",
    "                    pieceDF = pd.concat([pieceDF, dummyDF], axis=0)\n",
    "                \n",
    "                if well in list(df_solutions[\"Well_ID\"]):\n",
    "                    iWell = df_solutions.index[df_solutions[\"Well_ID\"]==well]\n",
    "                    if df_solutions.iloc[iWell,:][\"AL_Key\"].item()==pump:\n",
    "                        pieceDF=pieceDF[:-1]\n",
    "                                                \n",
    "                i=i+1\n",
    "        if j==0:\n",
    "            masterDF = pieceDF.copy(deep=True)\n",
    "        else:\n",
    "            masterDF = pd.concat([masterDF, pieceDF], axis=0)\n",
    "        j=j+1\n",
    "    return masterDF\n",
    "\n",
    "\n",
    "#Classification Model\n",
    "# def experiment_classification(models_dict):\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "#     scores={}\n",
    "#     for key in models_dict:\n",
    "#         scores[key] = \n",
    "    \n",
    "    \n",
    "\n",
    "### Plotting\n",
    "plot_window(df_daily, \"OIL\", 345)\n",
    "### Experimenting with Models\n",
    "df_daily = df_daily.drop([\"Unnamed: 0\"], axis=1)\n",
    "df_daily\n",
    "solution_df = pd.read_csv(\"solution.csv\")\n",
    "solution_df\n",
    "master_DF = build_dataset(df=df_daily, windowSize=30, df_solutions=solution_df, step=9);\n",
    "master_DF = pd.read_csv(\"class_dataset_panic.csv\")\n",
    "master_DF.shape\n",
    "master_DF.info()\n",
    "master_DF.columns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "\n",
    "class_fail = master_DF[(master_DF[\"fail\"] == 1)]\n",
    "print(class_fail.shape)\n",
    "#df_new_daily5[(df_new_daily5[\"ESP Data - Drive Voltage\"] > 250) & (df_new_daily5[\"GOR\"] < 8)]\n",
    "class_nofail = master_DF[(master_DF[\"fail\"] == 0)]\n",
    "print(class_nofail.shape)\n",
    "num_samples = len(class_fail)\n",
    "class_under_nofail = class_nofail.sample(len(class_fail), random_state=101)\n",
    "print(class_under_nofail.shape)\n",
    "master_DF_under = pd.concat([class_under_nofail, class_fail], axis=0)\n",
    "print(master_DF_under.shape)\n",
    "master_DF_under.head()\n",
    "master_DF_under.to_csv(\"classs_dataset_panic_update.csv\")\n",
    "len(class_fail)\n",
    "len(class_fail)\n",
    "master_DF.shape\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "\n",
    "class_fail = master_DF[(master_DF[\"fail\"] == 1)]\n",
    "print(class_fail.shape)\n",
    "#df_new_daily5[(df_new_daily5[\"ESP Data - Drive Voltage\"] > 250) & (df_new_daily5[\"GOR\"] < 8)]\n",
    "class_nofail = master_DF[(master_DF[\"fail\"] == 0)]\n",
    "print(class_nofail.shape)\n",
    "class_under_nofail = class_nofail.sample(88, random_state=101)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X=scaler.fit_transform(class_under_nofail)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    "    )\n",
    "#clf = make_pipeline(scaler,clf)\n",
    "\n",
    "#clf = SVC(class_weight='balanced', probability=True, C=0.1,kernel=\"rbf\", gamma=3)\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf=GridSearchCV(clf, {\"n_estimators\":[50,100,500],\n",
    "                       \"max_features\":[\"auto\", \"sqrt\"],\n",
    "                       \"max_depth\":[None,3,5,10],\n",
    "                       \"min_samples_split\":[2,3,5],\n",
    "                       \"min_samples_leaf\":[2,3,4],\n",
    "                       \"bootstrap\":[True,False],\n",
    "                       })\n",
    "\n",
    "clf.fit(X,y)\n",
    "clf.best_params_\n",
    "\n",
    "#cross_val_score(clf, X, y, cv=10)\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "#confusion_matrix(y_test, y_pred)\n",
    "y = master_DF[\"fail\"].values\n",
    "X = master_DF.drop([\"fail\"], axis=1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.33, random_state=42\n",
    "#     )\n",
    "#clf = make_pipeline(scaler,clf)\n",
    "\n",
    "#clf = SVC(class_weight='balanced', probability=True, C=0.1,kernel=\"rbf\", gamma=3)\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_features=\"auto\",\n",
    "                             max_depth=None,\n",
    "                             min_samples_split=2,\n",
    "                             min_samples_leaf=4,\n",
    "                             random_state=42)\n",
    "cross_val_score(clf,X,y,scoring=\"f1\")\n",
    "\n",
    "#cross_val_score(clf, X, y, cv=10)\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "#confusion_matrix(y_test, y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
